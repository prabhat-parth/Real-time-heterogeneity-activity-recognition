{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HETRO_WATCH_GYRO.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1KR35QUbqPAlPB8-MXfZmc_NKQQIpAHyU",
      "authorship_tag": "ABX9TyPWFe2cjYJ+i7RQr0sIjr3O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prabhat-parth/Real-time-heterogeneity-activity-recognition/blob/master/HETRO_WATCH_GYRO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3D-b3qEOtDuF",
        "colab_type": "code",
        "outputId": "faa0dcfb-27be-424d-c0a5-8efccda450a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "print(tf.__version__)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRSexPxlOr_t",
        "colab_type": "code",
        "outputId": "bb0a23b5-1b4c-4cab-edcc-6b52b170bb4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "data = pd.read_csv(\"/content/drive/My Drive/HETRO_DATA/Activity recognition exp/Activity recognition exp/Watch_gyroscope.csv\")\n",
        "data.head()\n",
        "data.shape\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3205431, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HwiYzUkPnRN",
        "colab_type": "code",
        "outputId": "052d3040-c41d-474f-f8dd-75240b0aaf21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Check for Duplicates\n",
        "print('No of duplicates in DATA: {}'.format(sum(data.duplicated())))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No of duplicates in DATA: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oratZmabQHat",
        "colab_type": "code",
        "outputId": "5bb7a0d5-5f36-409a-e3d4-545a42763ed9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Checking for NaN/null values\n",
        "print('We have {} NaN/Null values in data'.format(data.isnull().values.sum()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 470429 NaN/Null values in data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "97Hzdh7SSYch",
        "colab": {}
      },
      "source": [
        "#sns.heatmap(data.isnull(), cbar=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gv9vr8XJRtJe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.dropna(axis=0, how='any', thresh=None, subset=None, inplace= True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HaYimWq7u2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sns.heatmap(data.isnull(), cbar=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dQcmi0pvS5Rl",
        "outputId": "c2643754-3310-40bb-9636-8fc6a4d92111",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#Checking for NaN/null values\n",
        "print('We have {} NaN/Null values in data'.format(data.isnull().values.sum()))\n",
        "data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 0 NaN/Null values in data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2735002, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pF22Xd6knnC",
        "colab_type": "code",
        "outputId": "09fbeecb-7596-457d-dd1f-a12a51098747",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#data = data.drop(['Arrival_Time'], axis = 1).copy()\n",
        "data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2735002, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcNHiyd0JldN",
        "colab_type": "code",
        "outputId": "e3c3d56e-d37d-44d2-f172-95ce4c2efc9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Index</th>\n",
              "      <th>Arrival_Time</th>\n",
              "      <th>Creation_Time</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>z</th>\n",
              "      <th>User</th>\n",
              "      <th>Model</th>\n",
              "      <th>Device</th>\n",
              "      <th>gt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1424696638743</td>\n",
              "      <td>27920678496000</td>\n",
              "      <td>-0.162187</td>\n",
              "      <td>-0.022104</td>\n",
              "      <td>0.059655</td>\n",
              "      <td>a</td>\n",
              "      <td>gear</td>\n",
              "      <td>gear_1</td>\n",
              "      <td>stand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1424696638743</td>\n",
              "      <td>27920681926000</td>\n",
              "      <td>-0.183225</td>\n",
              "      <td>-0.061785</td>\n",
              "      <td>0.012517</td>\n",
              "      <td>a</td>\n",
              "      <td>gear</td>\n",
              "      <td>gear_1</td>\n",
              "      <td>stand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1424696638743</td>\n",
              "      <td>27920692031000</td>\n",
              "      <td>-0.180829</td>\n",
              "      <td>-0.108657</td>\n",
              "      <td>-0.036485</td>\n",
              "      <td>a</td>\n",
              "      <td>gear</td>\n",
              "      <td>gear_1</td>\n",
              "      <td>stand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1424696638743</td>\n",
              "      <td>27920701997000</td>\n",
              "      <td>-0.147805</td>\n",
              "      <td>-0.157925</td>\n",
              "      <td>-0.098537</td>\n",
              "      <td>a</td>\n",
              "      <td>gear</td>\n",
              "      <td>gear_1</td>\n",
              "      <td>stand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>1424696638744</td>\n",
              "      <td>27920743068000</td>\n",
              "      <td>0.182160</td>\n",
              "      <td>-0.323574</td>\n",
              "      <td>-0.277235</td>\n",
              "      <td>a</td>\n",
              "      <td>gear</td>\n",
              "      <td>gear_1</td>\n",
              "      <td>stand</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Index   Arrival_Time   Creation_Time         x  ...  User  Model  Device     gt\n",
              "0      0  1424696638743  27920678496000 -0.162187  ...     a   gear  gear_1  stand\n",
              "1      1  1424696638743  27920681926000 -0.183225  ...     a   gear  gear_1  stand\n",
              "2      2  1424696638743  27920692031000 -0.180829  ...     a   gear  gear_1  stand\n",
              "3      3  1424696638743  27920701997000 -0.147805  ...     a   gear  gear_1  stand\n",
              "4      7  1424696638744  27920743068000  0.182160  ...     a   gear  gear_1  stand\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZdYPJIjJo6I",
        "colab_type": "code",
        "outputId": "5365eda7-f48e-4b32-acc9-9f56043c926f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "#data = data.drop(['Index','Creation_Time','User','Model','Device'], axis = 1).copy()\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Index</th>\n",
              "      <th>Arrival_Time</th>\n",
              "      <th>Creation_Time</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>z</th>\n",
              "      <th>User</th>\n",
              "      <th>Model</th>\n",
              "      <th>Device</th>\n",
              "      <th>gt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1424696638743</td>\n",
              "      <td>27920678496000</td>\n",
              "      <td>-0.162187</td>\n",
              "      <td>-0.022104</td>\n",
              "      <td>0.059655</td>\n",
              "      <td>a</td>\n",
              "      <td>gear</td>\n",
              "      <td>gear_1</td>\n",
              "      <td>stand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1424696638743</td>\n",
              "      <td>27920681926000</td>\n",
              "      <td>-0.183225</td>\n",
              "      <td>-0.061785</td>\n",
              "      <td>0.012517</td>\n",
              "      <td>a</td>\n",
              "      <td>gear</td>\n",
              "      <td>gear_1</td>\n",
              "      <td>stand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1424696638743</td>\n",
              "      <td>27920692031000</td>\n",
              "      <td>-0.180829</td>\n",
              "      <td>-0.108657</td>\n",
              "      <td>-0.036485</td>\n",
              "      <td>a</td>\n",
              "      <td>gear</td>\n",
              "      <td>gear_1</td>\n",
              "      <td>stand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1424696638743</td>\n",
              "      <td>27920701997000</td>\n",
              "      <td>-0.147805</td>\n",
              "      <td>-0.157925</td>\n",
              "      <td>-0.098537</td>\n",
              "      <td>a</td>\n",
              "      <td>gear</td>\n",
              "      <td>gear_1</td>\n",
              "      <td>stand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>1424696638744</td>\n",
              "      <td>27920743068000</td>\n",
              "      <td>0.182160</td>\n",
              "      <td>-0.323574</td>\n",
              "      <td>-0.277235</td>\n",
              "      <td>a</td>\n",
              "      <td>gear</td>\n",
              "      <td>gear_1</td>\n",
              "      <td>stand</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Index   Arrival_Time   Creation_Time         x  ...  User  Model  Device     gt\n",
              "0      0  1424696638743  27920678496000 -0.162187  ...     a   gear  gear_1  stand\n",
              "1      1  1424696638743  27920681926000 -0.183225  ...     a   gear  gear_1  stand\n",
              "2      2  1424696638743  27920692031000 -0.180829  ...     a   gear  gear_1  stand\n",
              "3      3  1424696638743  27920701997000 -0.147805  ...     a   gear  gear_1  stand\n",
              "4      7  1424696638744  27920743068000  0.182160  ...     a   gear  gear_1  stand\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czgSYoh1ktNA",
        "colab_type": "code",
        "outputId": "0a8e12b8-f079-4a3a-f70c-2b4e150cca03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "data['gt'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "bike          522672\n",
              "walk          488309\n",
              "stairsup      446023\n",
              "stand         430223\n",
              "stairsdown    428241\n",
              "sit           419534\n",
              "Name: gt, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxCdtxAxk0t6",
        "colab_type": "code",
        "outputId": "6a4f6235-26bf-469f-b3af-8cc93190e50f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "source": [
        "data['x'] = data['x'].astype('float')\n",
        "data['y'] = data['y'].astype('float')\n",
        "data['z'] = data['z'].astype('float')\n",
        "data.info()\n",
        "data.shape\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 2735002 entries, 0 to 3205430\n",
            "Data columns (total 10 columns):\n",
            "Index            int64\n",
            "Arrival_Time     int64\n",
            "Creation_Time    int64\n",
            "x                float64\n",
            "y                float64\n",
            "z                float64\n",
            "User             object\n",
            "Model            object\n",
            "Device           object\n",
            "gt               object\n",
            "dtypes: float64(3), int64(3), object(4)\n",
            "memory usage: 229.5+ MB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2735002, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW9HWs0pk9vm",
        "colab_type": "code",
        "outputId": "ea641559-af16-4a2a-ee9b-928e1b1cae7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "Fs = 20\n",
        "gt= data['gt'].value_counts().index\n",
        "gt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['bike', 'walk', 'stairsup', 'stand', 'stairsdown', 'sit'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-Yg1SHFlZ-i",
        "colab_type": "code",
        "outputId": "cf877b5c-2487-40a4-862f-d38fff985a2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "label = LabelEncoder()\n",
        "data['label'] = label.fit_transform(data['gt'])\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Index</th>\n",
              "      <th>Arrival_Time</th>\n",
              "      <th>Creation_Time</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>z</th>\n",
              "      <th>User</th>\n",
              "      <th>Model</th>\n",
              "      <th>Device</th>\n",
              "      <th>gt</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1424696638743</td>\n",
              "      <td>27920678496000</td>\n",
              "      <td>-0.162187</td>\n",
              "      <td>-0.022104</td>\n",
              "      <td>0.059655</td>\n",
              "      <td>a</td>\n",
              "      <td>gear</td>\n",
              "      <td>gear_1</td>\n",
              "      <td>stand</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1424696638743</td>\n",
              "      <td>27920681926000</td>\n",
              "      <td>-0.183225</td>\n",
              "      <td>-0.061785</td>\n",
              "      <td>0.012517</td>\n",
              "      <td>a</td>\n",
              "      <td>gear</td>\n",
              "      <td>gear_1</td>\n",
              "      <td>stand</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1424696638743</td>\n",
              "      <td>27920692031000</td>\n",
              "      <td>-0.180829</td>\n",
              "      <td>-0.108657</td>\n",
              "      <td>-0.036485</td>\n",
              "      <td>a</td>\n",
              "      <td>gear</td>\n",
              "      <td>gear_1</td>\n",
              "      <td>stand</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1424696638743</td>\n",
              "      <td>27920701997000</td>\n",
              "      <td>-0.147805</td>\n",
              "      <td>-0.157925</td>\n",
              "      <td>-0.098537</td>\n",
              "      <td>a</td>\n",
              "      <td>gear</td>\n",
              "      <td>gear_1</td>\n",
              "      <td>stand</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>1424696638744</td>\n",
              "      <td>27920743068000</td>\n",
              "      <td>0.182160</td>\n",
              "      <td>-0.323574</td>\n",
              "      <td>-0.277235</td>\n",
              "      <td>a</td>\n",
              "      <td>gear</td>\n",
              "      <td>gear_1</td>\n",
              "      <td>stand</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Index   Arrival_Time   Creation_Time         x  ...  Model  Device     gt label\n",
              "0      0  1424696638743  27920678496000 -0.162187  ...   gear  gear_1  stand     4\n",
              "1      1  1424696638743  27920681926000 -0.183225  ...   gear  gear_1  stand     4\n",
              "2      2  1424696638743  27920692031000 -0.180829  ...   gear  gear_1  stand     4\n",
              "3      3  1424696638743  27920701997000 -0.147805  ...   gear  gear_1  stand     4\n",
              "4      7  1424696638744  27920743068000  0.182160  ...   gear  gear_1  stand     4\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voiP0mliljDm",
        "colab_type": "code",
        "outputId": "0f9cf8ee-821d-4626-d15b-418af75b251d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "label.classes_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['bike', 'sit', 'stairsdown', 'stairsup', 'stand', 'walk'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsLDkawYlmV4",
        "colab_type": "code",
        "outputId": "b08f23e2-1c33-4228-a80a-547a72ee724f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "X = data[['x', 'y', 'z']]\n",
        "y = data['label']\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "scaled_X = pd.DataFrame(data = X, columns = ['x', 'y', 'z'])\n",
        "scaled_X['label'] = y.values\n",
        "\n",
        "scaled_X"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>z</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.181342</td>\n",
              "      <td>-0.052623</td>\n",
              "      <td>0.077992</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.202538</td>\n",
              "      <td>-0.126329</td>\n",
              "      <td>0.030247</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.200124</td>\n",
              "      <td>-0.213392</td>\n",
              "      <td>-0.019386</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.166853</td>\n",
              "      <td>-0.304907</td>\n",
              "      <td>-0.082237</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.165582</td>\n",
              "      <td>-0.612594</td>\n",
              "      <td>-0.263237</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2734997</th>\n",
              "      <td>0.045334</td>\n",
              "      <td>0.307120</td>\n",
              "      <td>0.112140</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2734998</th>\n",
              "      <td>0.076541</td>\n",
              "      <td>0.235697</td>\n",
              "      <td>0.102403</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2734999</th>\n",
              "      <td>0.088378</td>\n",
              "      <td>0.170225</td>\n",
              "      <td>0.076438</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2735000</th>\n",
              "      <td>0.105596</td>\n",
              "      <td>0.144433</td>\n",
              "      <td>0.056965</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2735001</th>\n",
              "      <td>0.133575</td>\n",
              "      <td>0.172209</td>\n",
              "      <td>0.068865</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2735002 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                x         y         z  label\n",
              "0       -0.181342 -0.052623  0.077992      4\n",
              "1       -0.202538 -0.126329  0.030247      4\n",
              "2       -0.200124 -0.213392 -0.019386      4\n",
              "3       -0.166853 -0.304907 -0.082237      4\n",
              "4        0.165582 -0.612594 -0.263237      4\n",
              "...           ...       ...       ...    ...\n",
              "2734997  0.045334  0.307120  0.112140      0\n",
              "2734998  0.076541  0.235697  0.102403      0\n",
              "2734999  0.088378  0.170225  0.076438      0\n",
              "2735000  0.105596  0.144433  0.056965      0\n",
              "2735001  0.133575  0.172209  0.068865      0\n",
              "\n",
              "[2735002 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hi98Sixl7Ob",
        "colab_type": "code",
        "outputId": "81a6bb35-8f17-4ba2-bad9-c2886da60350",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "label.classes_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['bike', 'sit', 'stairsdown', 'stairsup', 'stand', 'walk'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkOBUiU01Rr7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Fs = 50\n",
        "frame_size = Fs*4 # 80\n",
        "hop_size = Fs*2 # 40\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIOtEbjOoH4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math as m\n",
        "def get_frames(data, frame_size, hop_size):\n",
        "\n",
        "  N_FEATURES = 3\n",
        "  frames = []\n",
        "  labels = []\n",
        "  for i in range(0, len(scaled_X) - frame_size, hop_size):\n",
        "    x = data['x'].values[i: i + frame_size]\n",
        "    y = data['y'].values[i: i + frame_size]\n",
        "    z = data['z'].values[i: i + frame_size]\n",
        "        # Retrieve the most often used label in this segment\n",
        "    label = stats.mode(scaled_X['label'][i: i + frame_size])[0][0]\n",
        "    frames.append([x, y, z])\n",
        "    labels.append(label)\n",
        "  # Bring the segments into a better shape\n",
        "  frames = np.asarray(frames, dtype= np.float32).reshape(-1, frame_size, N_FEATURES)\n",
        "  labels = np.asarray(labels)\n",
        "  return frames, labels\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZruOiWuCsJ-m",
        "colab": {}
      },
      "source": [
        "X,y = get_frames(scaled_X, frame_size, hop_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TlKwVuxQsJOX",
        "outputId": "056be057-2c7b-4a4c-e2ec-ce25d03a5eea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X.shape\n",
        "y.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(27349,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqf7zeTA2WiX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 0, stratify = y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4nRHfpq1Rwp",
        "colab_type": "code",
        "outputId": "8e36a2b2-5204-4f90-a9ce-0b01ec30055a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X_train.shape, X_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((21879, 200, 3), (5470, 200, 3))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2z45rOPQMRLK",
        "colab_type": "code",
        "outputId": "820d7a50-8a79-48ac-efed-6ed26826a56a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X_train = X_train.reshape(21879, 200, 3,1)\n",
        "X_test = X_test.reshape(5470, 200, 3,1)\n",
        "X_train[0].shape, X_test[0].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((200, 3, 1), (200, 3, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yx5ufXF5NWB-",
        "colab_type": "code",
        "outputId": "b7e9e731-dc2c-414c-b516-dcfb9495467c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Importing libraries\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.core import Dense, Dropout"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzkYLzMfdwXL",
        "colab_type": "code",
        "outputId": "0563084c-350c-4df3-e10d-f93a4d42a4c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu',input_shape=X_train[0].shape))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(6, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 200, 3, 64)        640       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 200, 3, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 100, 1, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 6400)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1024)              6554624   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 6)                 6150      \n",
            "=================================================================\n",
            "Total params: 6,561,414\n",
            "Trainable params: 6,561,414\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSRSMKjIecw4",
        "colab_type": "code",
        "outputId": "1005fd80-362c-4e3e-af15-dd412dd79f2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "from keras import optimizers\n",
        "sgd = optimizers.adam(lr=0.0001)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eX_iqGtZegEs",
        "colab_type": "code",
        "outputId": "737ed75f-6439-4531-cb75-98e9a86eba9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(X_train, y_train, epochs =30, validation_data= (X_test, y_test),batch_size=64, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 21879 samples, validate on 5470 samples\n",
            "Epoch 1/30\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "21879/21879 [==============================] - 17s 768us/step - loss: 1.2712 - acc: 0.4666 - val_loss: 1.0076 - val_acc: 0.5744\n",
            "Epoch 2/30\n",
            "21879/21879 [==============================] - 3s 115us/step - loss: 0.9409 - acc: 0.5985 - val_loss: 0.9127 - val_acc: 0.6331\n",
            "Epoch 3/30\n",
            "21879/21879 [==============================] - 3s 115us/step - loss: 0.8579 - acc: 0.6338 - val_loss: 0.8622 - val_acc: 0.6486\n",
            "Epoch 4/30\n",
            "21879/21879 [==============================] - 3s 114us/step - loss: 0.7978 - acc: 0.6626 - val_loss: 0.8321 - val_acc: 0.6691\n",
            "Epoch 5/30\n",
            "21879/21879 [==============================] - 2s 114us/step - loss: 0.7543 - acc: 0.6853 - val_loss: 0.7953 - val_acc: 0.6916\n",
            "Epoch 6/30\n",
            "21879/21879 [==============================] - 3s 114us/step - loss: 0.7133 - acc: 0.7013 - val_loss: 0.7708 - val_acc: 0.7000\n",
            "Epoch 7/30\n",
            "21879/21879 [==============================] - 3s 115us/step - loss: 0.6819 - acc: 0.7197 - val_loss: 0.7572 - val_acc: 0.7095\n",
            "Epoch 8/30\n",
            "21879/21879 [==============================] - 3s 116us/step - loss: 0.6547 - acc: 0.7302 - val_loss: 0.7406 - val_acc: 0.7124\n",
            "Epoch 9/30\n",
            "21879/21879 [==============================] - 3s 115us/step - loss: 0.6323 - acc: 0.7410 - val_loss: 0.7295 - val_acc: 0.7234\n",
            "Epoch 10/30\n",
            "21879/21879 [==============================] - 3s 115us/step - loss: 0.6076 - acc: 0.7526 - val_loss: 0.7265 - val_acc: 0.7278\n",
            "Epoch 11/30\n",
            "21879/21879 [==============================] - 3s 115us/step - loss: 0.5781 - acc: 0.7706 - val_loss: 0.7229 - val_acc: 0.7344\n",
            "Epoch 12/30\n",
            "21879/21879 [==============================] - 2s 114us/step - loss: 0.5677 - acc: 0.7706 - val_loss: 0.7042 - val_acc: 0.7371\n",
            "Epoch 13/30\n",
            "21879/21879 [==============================] - 3s 115us/step - loss: 0.5439 - acc: 0.7828 - val_loss: 0.6982 - val_acc: 0.7364\n",
            "Epoch 14/30\n",
            "21879/21879 [==============================] - 3s 116us/step - loss: 0.5273 - acc: 0.7881 - val_loss: 0.7031 - val_acc: 0.7283\n",
            "Epoch 15/30\n",
            "21879/21879 [==============================] - 3s 116us/step - loss: 0.5142 - acc: 0.7937 - val_loss: 0.6882 - val_acc: 0.7527\n",
            "Epoch 16/30\n",
            "21879/21879 [==============================] - 3s 117us/step - loss: 0.4939 - acc: 0.8042 - val_loss: 0.6964 - val_acc: 0.7400\n",
            "Epoch 17/30\n",
            "21879/21879 [==============================] - 3s 116us/step - loss: 0.4804 - acc: 0.8084 - val_loss: 0.6868 - val_acc: 0.7503\n",
            "Epoch 18/30\n",
            "21879/21879 [==============================] - 3s 118us/step - loss: 0.4678 - acc: 0.8159 - val_loss: 0.6778 - val_acc: 0.7530\n",
            "Epoch 19/30\n",
            "21879/21879 [==============================] - 3s 115us/step - loss: 0.4563 - acc: 0.8195 - val_loss: 0.6835 - val_acc: 0.7554\n",
            "Epoch 20/30\n",
            "21879/21879 [==============================] - 3s 116us/step - loss: 0.4439 - acc: 0.8233 - val_loss: 0.6792 - val_acc: 0.7601\n",
            "Epoch 21/30\n",
            "21879/21879 [==============================] - 2s 114us/step - loss: 0.4331 - acc: 0.8286 - val_loss: 0.6812 - val_acc: 0.7603\n",
            "Epoch 22/30\n",
            "21879/21879 [==============================] - 3s 115us/step - loss: 0.4201 - acc: 0.8353 - val_loss: 0.6796 - val_acc: 0.7572\n",
            "Epoch 23/30\n",
            "21879/21879 [==============================] - 2s 114us/step - loss: 0.4078 - acc: 0.8384 - val_loss: 0.6822 - val_acc: 0.7585\n",
            "Epoch 24/30\n",
            "21879/21879 [==============================] - 3s 115us/step - loss: 0.4006 - acc: 0.8378 - val_loss: 0.6720 - val_acc: 0.7607\n",
            "Epoch 25/30\n",
            "21879/21879 [==============================] - 2s 114us/step - loss: 0.3903 - acc: 0.8478 - val_loss: 0.6829 - val_acc: 0.7665\n",
            "Epoch 26/30\n",
            "21879/21879 [==============================] - 2s 114us/step - loss: 0.3790 - acc: 0.8499 - val_loss: 0.6809 - val_acc: 0.7634\n",
            "Epoch 27/30\n",
            "21879/21879 [==============================] - 3s 114us/step - loss: 0.3732 - acc: 0.8535 - val_loss: 0.6822 - val_acc: 0.7644\n",
            "Epoch 28/30\n",
            "21879/21879 [==============================] - 2s 113us/step - loss: 0.3660 - acc: 0.8573 - val_loss: 0.6817 - val_acc: 0.7612\n",
            "Epoch 29/30\n",
            "21879/21879 [==============================] - 2s 114us/step - loss: 0.3555 - acc: 0.8598 - val_loss: 0.6850 - val_acc: 0.7581\n",
            "Epoch 30/30\n",
            "21879/21879 [==============================] - 3s 114us/step - loss: 0.3453 - acc: 0.8609 - val_loss: 0.6852 - val_acc: 0.7709\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av3b7ZuloL82",
        "colab_type": "code",
        "outputId": "c27cec2b-948a-4efa-de94-af9df4ab9933",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 977
        }
      },
      "source": [
        "import matplotlib.pyplot as plt1\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.plot(history.history['acc'],label='Train acc')\n",
        "plt.plot(history.history['val_acc'],label = 'Validation acc')\n",
        "plt.xlabel('epoch no')\n",
        "plt.ylabel('acc')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.plot(history.history['loss'],label='Train loss')\n",
        "plt.plot(history.history['val_loss'],label = 'Validation loss')\n",
        "plt.xlabel('epoch no')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtcAAAHgCAYAAABuGUHVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSZKkghiuJUmSpIL8f1dhPXfR1pJvAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtEAAAHgCAYAAABjBzGSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJKlA/x+dka0RSiNb2QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQya7dGhoOlv",
        "colab_type": "code",
        "outputId": "c5ce1a9c-f10b-43ae-9f88-b92bcb657398",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        }
      },
      "source": [
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred = model.predict_classes(X_test)\n",
        "mat = confusion_matrix(y_test, y_pred)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(accuracy_score(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "\n",
        "plot_confusion_matrix(conf_mat=mat, show_normed=True, figsize=(7,7))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[993  14  14   5   8  11]\n",
            " [ 13 672   6   1 142   4]\n",
            " [ 33   4 646 103   3  68]\n",
            " [ 25   4 153 605   9  96]\n",
            " [ 29 288   6   6 522  10]\n",
            " [ 22  14  93  62   7 779]]\n",
            "0.7709323583180987\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.95      0.92      1045\n",
            "           1       0.67      0.80      0.73       838\n",
            "           2       0.70      0.75      0.73       857\n",
            "           3       0.77      0.68      0.72       892\n",
            "           4       0.76      0.61      0.67       861\n",
            "           5       0.80      0.80      0.80       977\n",
            "\n",
            "    accuracy                           0.77      5470\n",
            "   macro avg       0.77      0.76      0.76      5470\n",
            "weighted avg       0.77      0.77      0.77      5470\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<Figure size 504x504 with 1 Axes>,\n",
              " <matplotlib.axes._subplots.AxesSubplot at 0x7faa4006f9e8>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaYAAAGpCAYAAADGJ5LWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkhhEkIIoStSmIQQQujK/wBsK9iVeS6BEQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 504x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXjBJvwy5uiM",
        "colab_type": "code",
        "outputId": "cf02231f-c3cd-4234-f57f-0227046a366d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.metrics import classification_report\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from keras import optimizers\n",
        "\n",
        "fruits = [64,128,256,512,1024]\n",
        "for x in fruits:\n",
        "  print(x)\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu',input_shape=X_train[0].shape))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(MaxPooling2D(pool_size=2))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(1024, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(6, activation='softmax'))\n",
        "  model.summary()\n",
        "  sgd = optimizers.adam(lr=0.0001)\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "  history = model.fit(X_train, y_train, epochs =30, validation_data= (X_test, y_test),batch_size=x, verbose=1)  \n",
        "  y_pred = model.predict_classes(X_test)\n",
        "  mat = confusion_matrix(y_test, y_pred)\n",
        "  print(confusion_matrix(y_test, y_pred))\n",
        "  print(accuracy_score(y_test,y_pred))\n",
        "  print(classification_report(y_test,y_pred))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 200, 3, 64)        640       \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 200, 3, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 100, 1, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 6400)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1024)              6554624   \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 6)                 6150      \n",
            "=================================================================\n",
            "Total params: 6,561,414\n",
            "Trainable params: 6,561,414\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 21879 samples, validate on 5470 samples\n",
            "Epoch 1/30\n",
            "21879/21879 [==============================] - 3s 142us/step - loss: 1.2751 - acc: 0.4631 - val_loss: 1.0041 - val_acc: 0.6011\n",
            "Epoch 2/30\n",
            "21879/21879 [==============================] - 3s 117us/step - loss: 0.9383 - acc: 0.6023 - val_loss: 0.9192 - val_acc: 0.6216\n",
            "Epoch 3/30\n",
            "21879/21879 [==============================] - 3s 116us/step - loss: 0.8599 - acc: 0.6344 - val_loss: 0.8644 - val_acc: 0.6618\n",
            "Epoch 4/30\n",
            "21879/21879 [==============================] - 3s 116us/step - loss: 0.8046 - acc: 0.6613 - val_loss: 0.8320 - val_acc: 0.6576\n",
            "Epoch 5/30\n",
            "21879/21879 [==============================] - 3s 120us/step - loss: 0.7582 - acc: 0.6863 - val_loss: 0.8030 - val_acc: 0.6640\n",
            "Epoch 6/30\n",
            "21879/21879 [==============================] - 3s 122us/step - loss: 0.7182 - acc: 0.6996 - val_loss: 0.7831 - val_acc: 0.6973\n",
            "Epoch 7/30\n",
            "21879/21879 [==============================] - 3s 122us/step - loss: 0.6862 - acc: 0.7133 - val_loss: 0.7571 - val_acc: 0.7020\n",
            "Epoch 8/30\n",
            "21879/21879 [==============================] - 3s 121us/step - loss: 0.6592 - acc: 0.7323 - val_loss: 0.7414 - val_acc: 0.7170\n",
            "Epoch 9/30\n",
            "21879/21879 [==============================] - 2s 114us/step - loss: 0.6348 - acc: 0.7397 - val_loss: 0.7369 - val_acc: 0.7197\n",
            "Epoch 10/30\n",
            "21879/21879 [==============================] - 3s 115us/step - loss: 0.6137 - acc: 0.7477 - val_loss: 0.7197 - val_acc: 0.7258\n",
            "Epoch 11/30\n",
            "21879/21879 [==============================] - 3s 117us/step - loss: 0.5961 - acc: 0.7576 - val_loss: 0.7126 - val_acc: 0.7238\n",
            "Epoch 12/30\n",
            "21879/21879 [==============================] - 3s 115us/step - loss: 0.5713 - acc: 0.7669 - val_loss: 0.7149 - val_acc: 0.7419\n",
            "Epoch 13/30\n",
            "21879/21879 [==============================] - 3s 116us/step - loss: 0.5531 - acc: 0.7783 - val_loss: 0.6962 - val_acc: 0.7373\n",
            "Epoch 14/30\n",
            "21879/21879 [==============================] - 3s 115us/step - loss: 0.5380 - acc: 0.7839 - val_loss: 0.6866 - val_acc: 0.7391\n",
            "Epoch 15/30\n",
            "21879/21879 [==============================] - 3s 115us/step - loss: 0.5199 - acc: 0.7915 - val_loss: 0.6847 - val_acc: 0.7450\n",
            "Epoch 16/30\n",
            "21879/21879 [==============================] - 2s 114us/step - loss: 0.5036 - acc: 0.7971 - val_loss: 0.6805 - val_acc: 0.7397\n",
            "Epoch 17/30\n",
            "21879/21879 [==============================] - 2s 114us/step - loss: 0.4917 - acc: 0.8029 - val_loss: 0.6863 - val_acc: 0.7419\n",
            "Epoch 18/30\n",
            "21879/21879 [==============================] - 3s 115us/step - loss: 0.4746 - acc: 0.8095 - val_loss: 0.6827 - val_acc: 0.7439\n",
            "Epoch 19/30\n",
            "21879/21879 [==============================] - 3s 115us/step - loss: 0.4554 - acc: 0.8180 - val_loss: 0.6769 - val_acc: 0.7503\n",
            "Epoch 20/30\n",
            "21879/21879 [==============================] - 3s 115us/step - loss: 0.4499 - acc: 0.8227 - val_loss: 0.6736 - val_acc: 0.7479\n",
            "Epoch 21/30\n",
            "21879/21879 [==============================] - 3s 116us/step - loss: 0.4461 - acc: 0.8215 - val_loss: 0.6697 - val_acc: 0.7532\n",
            "Epoch 22/30\n",
            "21879/21879 [==============================] - 3s 115us/step - loss: 0.4259 - acc: 0.8327 - val_loss: 0.6737 - val_acc: 0.7550\n",
            "Epoch 23/30\n",
            "21879/21879 [==============================] - 3s 115us/step - loss: 0.4149 - acc: 0.8365 - val_loss: 0.6599 - val_acc: 0.7596\n",
            "Epoch 24/30\n",
            "21879/21879 [==============================] - 3s 116us/step - loss: 0.4026 - acc: 0.8420 - val_loss: 0.6750 - val_acc: 0.7596\n",
            "Epoch 25/30\n",
            "21879/21879 [==============================] - 3s 115us/step - loss: 0.3970 - acc: 0.8412 - val_loss: 0.6723 - val_acc: 0.7578\n",
            "Epoch 26/30\n",
            "21879/21879 [==============================] - 3s 116us/step - loss: 0.3834 - acc: 0.8490 - val_loss: 0.6732 - val_acc: 0.7673\n",
            "Epoch 27/30\n",
            "21879/21879 [==============================] - 3s 115us/step - loss: 0.3784 - acc: 0.8503 - val_loss: 0.6699 - val_acc: 0.7697\n",
            "Epoch 28/30\n",
            "21879/21879 [==============================] - 3s 115us/step - loss: 0.3698 - acc: 0.8542 - val_loss: 0.6683 - val_acc: 0.7633\n",
            "Epoch 29/30\n",
            "21879/21879 [==============================] - 3s 116us/step - loss: 0.3573 - acc: 0.8582 - val_loss: 0.6719 - val_acc: 0.7702\n",
            "Epoch 30/30\n",
            "21879/21879 [==============================] - 3s 121us/step - loss: 0.3524 - acc: 0.8632 - val_loss: 0.6791 - val_acc: 0.7620\n",
            "[[978  16  18   9   8  16]\n",
            " [ 13 688   5   3 124   5]\n",
            " [ 25   5 631 128   2  66]\n",
            " [ 19   4 146 625   7  91]\n",
            " [ 23 327   8   8 482  13]\n",
            " [  9  13  92  92   7 764]]\n",
            "0.7619744058500915\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.94      0.93      1045\n",
            "           1       0.65      0.82      0.73       838\n",
            "           2       0.70      0.74      0.72       857\n",
            "           3       0.72      0.70      0.71       892\n",
            "           4       0.77      0.56      0.65       861\n",
            "           5       0.80      0.78      0.79       977\n",
            "\n",
            "    accuracy                           0.76      5470\n",
            "   macro avg       0.76      0.76      0.75      5470\n",
            "weighted avg       0.77      0.76      0.76      5470\n",
            "\n",
            "128\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 200, 3, 64)        640       \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 200, 3, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 100, 1, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 6400)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1024)              6554624   \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 6)                 6150      \n",
            "=================================================================\n",
            "Total params: 6,561,414\n",
            "Trainable params: 6,561,414\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 21879 samples, validate on 5470 samples\n",
            "Epoch 1/30\n",
            "21879/21879 [==============================] - 2s 103us/step - loss: 1.4283 - acc: 0.4107 - val_loss: 1.1991 - val_acc: 0.5722\n",
            "Epoch 2/30\n",
            "21879/21879 [==============================] - 2s 69us/step - loss: 1.0080 - acc: 0.5858 - val_loss: 0.9666 - val_acc: 0.6174\n",
            "Epoch 3/30\n",
            "21879/21879 [==============================] - 2s 70us/step - loss: 0.8972 - acc: 0.6237 - val_loss: 0.9049 - val_acc: 0.6400\n",
            "Epoch 4/30\n",
            "21879/21879 [==============================] - 1s 68us/step - loss: 0.8367 - acc: 0.6526 - val_loss: 0.8740 - val_acc: 0.6572\n",
            "Epoch 5/30\n",
            "21879/21879 [==============================] - 2s 69us/step - loss: 0.7958 - acc: 0.6667 - val_loss: 0.8345 - val_acc: 0.6737\n",
            "Epoch 6/30\n",
            "21879/21879 [==============================] - 2s 70us/step - loss: 0.7581 - acc: 0.6817 - val_loss: 0.8108 - val_acc: 0.7073\n",
            "Epoch 7/30\n",
            "21879/21879 [==============================] - 2s 70us/step - loss: 0.7277 - acc: 0.6995 - val_loss: 0.7948 - val_acc: 0.6978\n",
            "Epoch 8/30\n",
            "21879/21879 [==============================] - 2s 70us/step - loss: 0.6960 - acc: 0.7102 - val_loss: 0.7756 - val_acc: 0.7059\n",
            "Epoch 9/30\n",
            "21879/21879 [==============================] - 2s 69us/step - loss: 0.6723 - acc: 0.7274 - val_loss: 0.7614 - val_acc: 0.7166\n",
            "Epoch 10/30\n",
            "21879/21879 [==============================] - 1s 68us/step - loss: 0.6501 - acc: 0.7331 - val_loss: 0.7595 - val_acc: 0.7179\n",
            "Epoch 11/30\n",
            "21879/21879 [==============================] - 2s 69us/step - loss: 0.6277 - acc: 0.7467 - val_loss: 0.7463 - val_acc: 0.7144\n",
            "Epoch 12/30\n",
            "21879/21879 [==============================] - 2s 69us/step - loss: 0.6093 - acc: 0.7537 - val_loss: 0.7379 - val_acc: 0.7300\n",
            "Epoch 13/30\n",
            "21879/21879 [==============================] - 2s 69us/step - loss: 0.5903 - acc: 0.7615 - val_loss: 0.7287 - val_acc: 0.7247\n",
            "Epoch 14/30\n",
            "21879/21879 [==============================] - 2s 69us/step - loss: 0.5759 - acc: 0.7653 - val_loss: 0.7192 - val_acc: 0.7283\n",
            "Epoch 15/30\n",
            "21879/21879 [==============================] - 2s 69us/step - loss: 0.5615 - acc: 0.7737 - val_loss: 0.7172 - val_acc: 0.7327\n",
            "Epoch 16/30\n",
            "21879/21879 [==============================] - 1s 68us/step - loss: 0.5447 - acc: 0.7789 - val_loss: 0.7122 - val_acc: 0.7344\n",
            "Epoch 17/30\n",
            "21879/21879 [==============================] - 1s 68us/step - loss: 0.5306 - acc: 0.7877 - val_loss: 0.7121 - val_acc: 0.7420\n",
            "Epoch 18/30\n",
            "21879/21879 [==============================] - 2s 70us/step - loss: 0.5169 - acc: 0.7948 - val_loss: 0.7016 - val_acc: 0.7373\n",
            "Epoch 19/30\n",
            "21879/21879 [==============================] - 2s 70us/step - loss: 0.5067 - acc: 0.8004 - val_loss: 0.7022 - val_acc: 0.7380\n",
            "Epoch 20/30\n",
            "21879/21879 [==============================] - 2s 69us/step - loss: 0.4915 - acc: 0.8055 - val_loss: 0.6937 - val_acc: 0.7506\n",
            "Epoch 21/30\n",
            "21879/21879 [==============================] - 2s 70us/step - loss: 0.4811 - acc: 0.8110 - val_loss: 0.6967 - val_acc: 0.7444\n",
            "Epoch 22/30\n",
            "21879/21879 [==============================] - 1s 68us/step - loss: 0.4704 - acc: 0.8126 - val_loss: 0.6945 - val_acc: 0.7503\n",
            "Epoch 23/30\n",
            "21879/21879 [==============================] - 2s 69us/step - loss: 0.4609 - acc: 0.8142 - val_loss: 0.6998 - val_acc: 0.7501\n",
            "Epoch 24/30\n",
            "21879/21879 [==============================] - 2s 69us/step - loss: 0.4470 - acc: 0.8215 - val_loss: 0.6908 - val_acc: 0.7539\n",
            "Epoch 25/30\n",
            "21879/21879 [==============================] - 1s 68us/step - loss: 0.4390 - acc: 0.8269 - val_loss: 0.6919 - val_acc: 0.7574\n",
            "Epoch 26/30\n",
            "21879/21879 [==============================] - 1s 68us/step - loss: 0.4306 - acc: 0.8284 - val_loss: 0.7001 - val_acc: 0.7558\n",
            "Epoch 27/30\n",
            "21879/21879 [==============================] - 2s 69us/step - loss: 0.4189 - acc: 0.8344 - val_loss: 0.6868 - val_acc: 0.7592\n",
            "Epoch 28/30\n",
            "21879/21879 [==============================] - 2s 69us/step - loss: 0.4121 - acc: 0.8366 - val_loss: 0.6889 - val_acc: 0.7629\n",
            "Epoch 29/30\n",
            "21879/21879 [==============================] - 2s 69us/step - loss: 0.4068 - acc: 0.8374 - val_loss: 0.6909 - val_acc: 0.7592\n",
            "Epoch 30/30\n",
            "21879/21879 [==============================] - 2s 70us/step - loss: 0.4003 - acc: 0.8406 - val_loss: 0.6808 - val_acc: 0.7634\n",
            "[[990  15  16   8   5  11]\n",
            " [ 15 657   6   3 154   3]\n",
            " [ 32   3 608 158   4  52]\n",
            " [ 20   4 124 658   6  80]\n",
            " [ 26 319   7   8 492   9]\n",
            " [ 23  12  75  90   6 771]]\n",
            "0.763436928702011\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.95      0.92      1045\n",
            "           1       0.65      0.78      0.71       838\n",
            "           2       0.73      0.71      0.72       857\n",
            "           3       0.71      0.74      0.72       892\n",
            "           4       0.74      0.57      0.64       861\n",
            "           5       0.83      0.79      0.81       977\n",
            "\n",
            "    accuracy                           0.76      5470\n",
            "   macro avg       0.76      0.76      0.75      5470\n",
            "weighted avg       0.77      0.76      0.76      5470\n",
            "\n",
            "256\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 200, 3, 64)        640       \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 200, 3, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 100, 1, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 6400)              0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1024)              6554624   \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 6)                 6150      \n",
            "=================================================================\n",
            "Total params: 6,561,414\n",
            "Trainable params: 6,561,414\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 21879 samples, validate on 5470 samples\n",
            "Epoch 1/30\n",
            "21879/21879 [==============================] - 2s 76us/step - loss: 1.4955 - acc: 0.3211 - val_loss: 1.3694 - val_acc: 0.4119\n",
            "Epoch 2/30\n",
            "21879/21879 [==============================] - 1s 45us/step - loss: 1.2130 - acc: 0.5168 - val_loss: 1.0941 - val_acc: 0.5951\n",
            "Epoch 3/30\n",
            "21879/21879 [==============================] - 1s 45us/step - loss: 0.9732 - acc: 0.6056 - val_loss: 0.9612 - val_acc: 0.6378\n",
            "Epoch 4/30\n",
            "21879/21879 [==============================] - 1s 45us/step - loss: 0.8891 - acc: 0.6317 - val_loss: 0.9060 - val_acc: 0.6501\n",
            "Epoch 5/30\n",
            "21879/21879 [==============================] - 1s 45us/step - loss: 0.8381 - acc: 0.6551 - val_loss: 0.8709 - val_acc: 0.6636\n",
            "Epoch 6/30\n",
            "21879/21879 [==============================] - 1s 46us/step - loss: 0.8013 - acc: 0.6691 - val_loss: 0.8456 - val_acc: 0.6651\n",
            "Epoch 7/30\n",
            "21879/21879 [==============================] - 1s 45us/step - loss: 0.7713 - acc: 0.6774 - val_loss: 0.8286 - val_acc: 0.6790\n",
            "Epoch 8/30\n",
            "21879/21879 [==============================] - 1s 45us/step - loss: 0.7450 - acc: 0.6936 - val_loss: 0.8149 - val_acc: 0.6899\n",
            "Epoch 9/30\n",
            "21879/21879 [==============================] - 1s 45us/step - loss: 0.7212 - acc: 0.7077 - val_loss: 0.7954 - val_acc: 0.7000\n",
            "Epoch 10/30\n",
            "21879/21879 [==============================] - 1s 45us/step - loss: 0.7020 - acc: 0.7142 - val_loss: 0.7834 - val_acc: 0.6965\n",
            "Epoch 11/30\n",
            "21879/21879 [==============================] - 1s 45us/step - loss: 0.6854 - acc: 0.7195 - val_loss: 0.7699 - val_acc: 0.7048\n",
            "Epoch 12/30\n",
            "21879/21879 [==============================] - 1s 45us/step - loss: 0.6694 - acc: 0.7262 - val_loss: 0.7635 - val_acc: 0.7099\n",
            "Epoch 13/30\n",
            "21879/21879 [==============================] - 1s 46us/step - loss: 0.6496 - acc: 0.7339 - val_loss: 0.7523 - val_acc: 0.7117\n",
            "Epoch 14/30\n",
            "21879/21879 [==============================] - 1s 46us/step - loss: 0.6312 - acc: 0.7432 - val_loss: 0.7421 - val_acc: 0.7256\n",
            "Epoch 15/30\n",
            "21879/21879 [==============================] - 1s 46us/step - loss: 0.6196 - acc: 0.7518 - val_loss: 0.7437 - val_acc: 0.7121\n",
            "Epoch 16/30\n",
            "21879/21879 [==============================] - 1s 45us/step - loss: 0.6070 - acc: 0.7548 - val_loss: 0.7369 - val_acc: 0.7141\n",
            "Epoch 17/30\n",
            "21879/21879 [==============================] - 1s 45us/step - loss: 0.5920 - acc: 0.7605 - val_loss: 0.7307 - val_acc: 0.7245\n",
            "Epoch 18/30\n",
            "21879/21879 [==============================] - 1s 45us/step - loss: 0.5847 - acc: 0.7660 - val_loss: 0.7307 - val_acc: 0.7250\n",
            "Epoch 19/30\n",
            "21879/21879 [==============================] - 1s 44us/step - loss: 0.5673 - acc: 0.7735 - val_loss: 0.7266 - val_acc: 0.7250\n",
            "Epoch 20/30\n",
            "21879/21879 [==============================] - 1s 44us/step - loss: 0.5619 - acc: 0.7740 - val_loss: 0.7259 - val_acc: 0.7300\n",
            "Epoch 21/30\n",
            "21879/21879 [==============================] - 1s 45us/step - loss: 0.5504 - acc: 0.7780 - val_loss: 0.7195 - val_acc: 0.7305\n",
            "Epoch 22/30\n",
            "21879/21879 [==============================] - 1s 46us/step - loss: 0.5415 - acc: 0.7797 - val_loss: 0.7162 - val_acc: 0.7349\n",
            "Epoch 23/30\n",
            "21879/21879 [==============================] - 1s 45us/step - loss: 0.5306 - acc: 0.7876 - val_loss: 0.7163 - val_acc: 0.7316\n",
            "Epoch 24/30\n",
            "21879/21879 [==============================] - 1s 45us/step - loss: 0.5210 - acc: 0.7931 - val_loss: 0.7128 - val_acc: 0.7353\n",
            "Epoch 25/30\n",
            "21879/21879 [==============================] - 1s 45us/step - loss: 0.5054 - acc: 0.7975 - val_loss: 0.6985 - val_acc: 0.7353\n",
            "Epoch 26/30\n",
            "21879/21879 [==============================] - 1s 45us/step - loss: 0.5000 - acc: 0.7995 - val_loss: 0.7048 - val_acc: 0.7400\n",
            "Epoch 27/30\n",
            "21879/21879 [==============================] - 1s 45us/step - loss: 0.4955 - acc: 0.8042 - val_loss: 0.7058 - val_acc: 0.7300\n",
            "Epoch 28/30\n",
            "21879/21879 [==============================] - 1s 45us/step - loss: 0.4827 - acc: 0.8071 - val_loss: 0.7002 - val_acc: 0.7428\n",
            "Epoch 29/30\n",
            "21879/21879 [==============================] - 1s 45us/step - loss: 0.4711 - acc: 0.8130 - val_loss: 0.6937 - val_acc: 0.7411\n",
            "Epoch 30/30\n",
            "21879/21879 [==============================] - 1s 45us/step - loss: 0.4654 - acc: 0.8156 - val_loss: 0.6911 - val_acc: 0.7495\n",
            "[[995  13  11   5   9  12]\n",
            " [ 16 604   6   1 208   3]\n",
            " [ 34   2 615 134   3  69]\n",
            " [ 26   4 139 605   6 112]\n",
            " [ 29 295   7   7 510  13]\n",
            " [ 22  13  81  84   6 771]]\n",
            "0.7495429616087751\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.95      0.92      1045\n",
            "           1       0.65      0.72      0.68       838\n",
            "           2       0.72      0.72      0.72       857\n",
            "           3       0.72      0.68      0.70       892\n",
            "           4       0.69      0.59      0.64       861\n",
            "           5       0.79      0.79      0.79       977\n",
            "\n",
            "    accuracy                           0.75      5470\n",
            "   macro avg       0.74      0.74      0.74      5470\n",
            "weighted avg       0.75      0.75      0.75      5470\n",
            "\n",
            "512\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_5 (Conv2D)            (None, 200, 3, 64)        640       \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 200, 3, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 100, 1, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 6400)              0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1024)              6554624   \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 6)                 6150      \n",
            "=================================================================\n",
            "Total params: 6,561,414\n",
            "Trainable params: 6,561,414\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 21879 samples, validate on 5470 samples\n",
            "Epoch 1/30\n",
            "21879/21879 [==============================] - 2s 69us/step - loss: 1.5473 - acc: 0.2914 - val_loss: 1.4530 - val_acc: 0.4272\n",
            "Epoch 2/30\n",
            "21879/21879 [==============================] - 1s 33us/step - loss: 1.3844 - acc: 0.3939 - val_loss: 1.3334 - val_acc: 0.4307\n",
            "Epoch 3/30\n",
            "21879/21879 [==============================] - 1s 33us/step - loss: 1.2210 - acc: 0.5254 - val_loss: 1.1589 - val_acc: 0.5896\n",
            "Epoch 4/30\n",
            "21879/21879 [==============================] - 1s 33us/step - loss: 1.0203 - acc: 0.6002 - val_loss: 1.0102 - val_acc: 0.6177\n",
            "Epoch 5/30\n",
            "21879/21879 [==============================] - 1s 33us/step - loss: 0.9279 - acc: 0.6208 - val_loss: 0.9513 - val_acc: 0.6338\n",
            "Epoch 6/30\n",
            "21879/21879 [==============================] - 1s 33us/step - loss: 0.8826 - acc: 0.6376 - val_loss: 0.9122 - val_acc: 0.6508\n",
            "Epoch 7/30\n",
            "21879/21879 [==============================] - 1s 33us/step - loss: 0.8449 - acc: 0.6505 - val_loss: 0.8883 - val_acc: 0.6572\n",
            "Epoch 8/30\n",
            "21879/21879 [==============================] - 1s 33us/step - loss: 0.8126 - acc: 0.6679 - val_loss: 0.8643 - val_acc: 0.6622\n",
            "Epoch 9/30\n",
            "21879/21879 [==============================] - 1s 33us/step - loss: 0.7902 - acc: 0.6711 - val_loss: 0.8486 - val_acc: 0.6748\n",
            "Epoch 10/30\n",
            "21879/21879 [==============================] - 1s 33us/step - loss: 0.7669 - acc: 0.6833 - val_loss: 0.8279 - val_acc: 0.6841\n",
            "Epoch 11/30\n",
            "21879/21879 [==============================] - 1s 33us/step - loss: 0.7440 - acc: 0.6929 - val_loss: 0.8183 - val_acc: 0.6965\n",
            "Epoch 12/30\n",
            "21879/21879 [==============================] - 1s 33us/step - loss: 0.7355 - acc: 0.6942 - val_loss: 0.8085 - val_acc: 0.6883\n",
            "Epoch 13/30\n",
            "21879/21879 [==============================] - 1s 33us/step - loss: 0.7135 - acc: 0.7074 - val_loss: 0.8005 - val_acc: 0.6916\n",
            "Epoch 14/30\n",
            "21879/21879 [==============================] - 1s 33us/step - loss: 0.6996 - acc: 0.7139 - val_loss: 0.7854 - val_acc: 0.6978\n",
            "Epoch 15/30\n",
            "21879/21879 [==============================] - 1s 33us/step - loss: 0.6847 - acc: 0.7196 - val_loss: 0.7833 - val_acc: 0.6984\n",
            "Epoch 16/30\n",
            "21879/21879 [==============================] - 1s 33us/step - loss: 0.6716 - acc: 0.7237 - val_loss: 0.7687 - val_acc: 0.7007\n",
            "Epoch 17/30\n",
            "21879/21879 [==============================] - 1s 34us/step - loss: 0.6605 - acc: 0.7298 - val_loss: 0.7665 - val_acc: 0.7038\n",
            "Epoch 18/30\n",
            "21879/21879 [==============================] - 1s 33us/step - loss: 0.6469 - acc: 0.7329 - val_loss: 0.7638 - val_acc: 0.7099\n",
            "Epoch 19/30\n",
            "21879/21879 [==============================] - 1s 33us/step - loss: 0.6348 - acc: 0.7429 - val_loss: 0.7558 - val_acc: 0.7124\n",
            "Epoch 20/30\n",
            "21879/21879 [==============================] - 1s 33us/step - loss: 0.6259 - acc: 0.7464 - val_loss: 0.7513 - val_acc: 0.7119\n",
            "Epoch 21/30\n",
            "21879/21879 [==============================] - 1s 33us/step - loss: 0.6144 - acc: 0.7499 - val_loss: 0.7437 - val_acc: 0.7135\n",
            "Epoch 22/30\n",
            "21879/21879 [==============================] - 1s 33us/step - loss: 0.6066 - acc: 0.7552 - val_loss: 0.7410 - val_acc: 0.7152\n",
            "Epoch 23/30\n",
            "21879/21879 [==============================] - 1s 33us/step - loss: 0.6001 - acc: 0.7576 - val_loss: 0.7400 - val_acc: 0.7152\n",
            "Epoch 24/30\n",
            "21879/21879 [==============================] - 1s 33us/step - loss: 0.5847 - acc: 0.7624 - val_loss: 0.7318 - val_acc: 0.7196\n",
            "Epoch 25/30\n",
            "21879/21879 [==============================] - 1s 33us/step - loss: 0.5751 - acc: 0.7726 - val_loss: 0.7310 - val_acc: 0.7263\n",
            "Epoch 26/30\n",
            "21879/21879 [==============================] - 1s 33us/step - loss: 0.5700 - acc: 0.7706 - val_loss: 0.7276 - val_acc: 0.7274\n",
            "Epoch 27/30\n",
            "21879/21879 [==============================] - 1s 33us/step - loss: 0.5604 - acc: 0.7738 - val_loss: 0.7267 - val_acc: 0.7287\n",
            "Epoch 28/30\n",
            "21879/21879 [==============================] - 1s 34us/step - loss: 0.5503 - acc: 0.7781 - val_loss: 0.7199 - val_acc: 0.7335\n",
            "Epoch 29/30\n",
            "21879/21879 [==============================] - 1s 33us/step - loss: 0.5444 - acc: 0.7839 - val_loss: 0.7182 - val_acc: 0.7340\n",
            "Epoch 30/30\n",
            "21879/21879 [==============================] - 1s 33us/step - loss: 0.5347 - acc: 0.7864 - val_loss: 0.7199 - val_acc: 0.7309\n",
            "[[999  13  11   8   7   7]\n",
            " [ 17 637   5   0 178   1]\n",
            " [ 38   4 615 134   2  64]\n",
            " [ 26   4 152 586   9 115]\n",
            " [ 34 377   4   5 427  14]\n",
            " [ 33  14  94  95   7 734]]\n",
            "0.7308957952468007\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.96      0.91      1045\n",
            "           1       0.61      0.76      0.68       838\n",
            "           2       0.70      0.72      0.71       857\n",
            "           3       0.71      0.66      0.68       892\n",
            "           4       0.68      0.50      0.57       861\n",
            "           5       0.79      0.75      0.77       977\n",
            "\n",
            "    accuracy                           0.73      5470\n",
            "   macro avg       0.72      0.72      0.72      5470\n",
            "weighted avg       0.73      0.73      0.73      5470\n",
            "\n",
            "1024\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 200, 3, 64)        640       \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 200, 3, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 100, 1, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 6400)              0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1024)              6554624   \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 6)                 6150      \n",
            "=================================================================\n",
            "Total params: 6,561,414\n",
            "Trainable params: 6,561,414\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 21879 samples, validate on 5470 samples\n",
            "Epoch 1/30\n",
            "21879/21879 [==============================] - 1s 66us/step - loss: 1.5903 - acc: 0.2425 - val_loss: 1.5109 - val_acc: 0.3711\n",
            "Epoch 2/30\n",
            "21879/21879 [==============================] - 1s 28us/step - loss: 1.4501 - acc: 0.3606 - val_loss: 1.4149 - val_acc: 0.4035\n",
            "Epoch 3/30\n",
            "21879/21879 [==============================] - 1s 28us/step - loss: 1.3568 - acc: 0.4045 - val_loss: 1.3379 - val_acc: 0.4424\n",
            "Epoch 4/30\n",
            "21879/21879 [==============================] - 1s 28us/step - loss: 1.2708 - acc: 0.4428 - val_loss: 1.2591 - val_acc: 0.5238\n",
            "Epoch 5/30\n",
            "21879/21879 [==============================] - 1s 28us/step - loss: 1.1706 - acc: 0.5480 - val_loss: 1.1688 - val_acc: 0.6026\n",
            "Epoch 6/30\n",
            "21879/21879 [==============================] - 1s 28us/step - loss: 1.0542 - acc: 0.6131 - val_loss: 1.0692 - val_acc: 0.6197\n",
            "Epoch 7/30\n",
            "21879/21879 [==============================] - 1s 28us/step - loss: 0.9650 - acc: 0.6222 - val_loss: 0.9989 - val_acc: 0.6342\n",
            "Epoch 8/30\n",
            "21879/21879 [==============================] - 1s 28us/step - loss: 0.9114 - acc: 0.6371 - val_loss: 0.9518 - val_acc: 0.6475\n",
            "Epoch 9/30\n",
            "21879/21879 [==============================] - 1s 28us/step - loss: 0.8743 - acc: 0.6507 - val_loss: 0.9251 - val_acc: 0.6488\n",
            "Epoch 10/30\n",
            "21879/21879 [==============================] - 1s 28us/step - loss: 0.8456 - acc: 0.6578 - val_loss: 0.9050 - val_acc: 0.6547\n",
            "Epoch 11/30\n",
            "21879/21879 [==============================] - 1s 28us/step - loss: 0.8215 - acc: 0.6621 - val_loss: 0.8871 - val_acc: 0.6618\n",
            "Epoch 12/30\n",
            "21879/21879 [==============================] - 1s 27us/step - loss: 0.7980 - acc: 0.6759 - val_loss: 0.8652 - val_acc: 0.6697\n",
            "Epoch 13/30\n",
            "21879/21879 [==============================] - 1s 27us/step - loss: 0.7882 - acc: 0.6738 - val_loss: 0.8559 - val_acc: 0.6753\n",
            "Epoch 14/30\n",
            "21879/21879 [==============================] - 1s 27us/step - loss: 0.7674 - acc: 0.6902 - val_loss: 0.8456 - val_acc: 0.6761\n",
            "Epoch 15/30\n",
            "21879/21879 [==============================] - 1s 28us/step - loss: 0.7496 - acc: 0.6919 - val_loss: 0.8337 - val_acc: 0.6819\n",
            "Epoch 16/30\n",
            "21879/21879 [==============================] - 1s 28us/step - loss: 0.7399 - acc: 0.6967 - val_loss: 0.8230 - val_acc: 0.6870\n",
            "Epoch 17/30\n",
            "21879/21879 [==============================] - 1s 28us/step - loss: 0.7244 - acc: 0.7049 - val_loss: 0.8127 - val_acc: 0.6956\n",
            "Epoch 18/30\n",
            "21879/21879 [==============================] - 1s 28us/step - loss: 0.7101 - acc: 0.7106 - val_loss: 0.8074 - val_acc: 0.6899\n",
            "Epoch 19/30\n",
            "21879/21879 [==============================] - 1s 28us/step - loss: 0.6988 - acc: 0.7151 - val_loss: 0.7980 - val_acc: 0.7000\n",
            "Epoch 20/30\n",
            "21879/21879 [==============================] - 1s 27us/step - loss: 0.6903 - acc: 0.7206 - val_loss: 0.7986 - val_acc: 0.7033\n",
            "Epoch 21/30\n",
            "21879/21879 [==============================] - 1s 27us/step - loss: 0.6808 - acc: 0.7222 - val_loss: 0.7881 - val_acc: 0.6993\n",
            "Epoch 22/30\n",
            "21879/21879 [==============================] - 1s 28us/step - loss: 0.6759 - acc: 0.7259 - val_loss: 0.7878 - val_acc: 0.7009\n",
            "Epoch 23/30\n",
            "21879/21879 [==============================] - 1s 27us/step - loss: 0.6622 - acc: 0.7335 - val_loss: 0.7801 - val_acc: 0.7015\n",
            "Epoch 24/30\n",
            "21879/21879 [==============================] - 1s 28us/step - loss: 0.6508 - acc: 0.7372 - val_loss: 0.7764 - val_acc: 0.7080\n",
            "Epoch 25/30\n",
            "21879/21879 [==============================] - 1s 28us/step - loss: 0.6402 - acc: 0.7431 - val_loss: 0.7648 - val_acc: 0.7197\n",
            "Epoch 26/30\n",
            "21879/21879 [==============================] - 1s 28us/step - loss: 0.6370 - acc: 0.7433 - val_loss: 0.7659 - val_acc: 0.7086\n",
            "Epoch 27/30\n",
            "21879/21879 [==============================] - 1s 28us/step - loss: 0.6267 - acc: 0.7491 - val_loss: 0.7622 - val_acc: 0.7150\n",
            "Epoch 28/30\n",
            "21879/21879 [==============================] - 1s 28us/step - loss: 0.6199 - acc: 0.7532 - val_loss: 0.7564 - val_acc: 0.7190\n",
            "Epoch 29/30\n",
            "21879/21879 [==============================] - 1s 28us/step - loss: 0.6128 - acc: 0.7543 - val_loss: 0.7555 - val_acc: 0.7161\n",
            "Epoch 30/30\n",
            "21879/21879 [==============================] - 1s 28us/step - loss: 0.6053 - acc: 0.7562 - val_loss: 0.7521 - val_acc: 0.7172\n",
            "[[998  12   9   6   9  11]\n",
            " [ 22 621   3   1 190   1]\n",
            " [ 52   3 557 152   1  92]\n",
            " [ 30   4 118 547   4 189]\n",
            " [ 49 378   4   5 414  11]\n",
            " [ 41  11  60  71   8 786]]\n",
            "0.7171846435100548\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.96      0.89      1045\n",
            "           1       0.60      0.74      0.67       838\n",
            "           2       0.74      0.65      0.69       857\n",
            "           3       0.70      0.61      0.65       892\n",
            "           4       0.66      0.48      0.56       861\n",
            "           5       0.72      0.80      0.76       977\n",
            "\n",
            "    accuracy                           0.72      5470\n",
            "   macro avg       0.71      0.71      0.70      5470\n",
            "weighted avg       0.72      0.72      0.71      5470\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GO95nZfz52IF",
        "colab_type": "code",
        "outputId": "a00aa408-e41c-4cbf-bdbb-fba92482e806",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.metrics import classification_report\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from keras import optimizers\n",
        "\n",
        "fruits = [0.0001,0.001,0.01,0.1,1.0]\n",
        "for x in fruits:\n",
        "  print(x)\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu',input_shape=X_train[0].shape))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(MaxPooling2D(pool_size=2))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(1024, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(6, activation='softmax'))\n",
        "  model.summary() \n",
        "  sgd = optimizers.adam(lr=x)\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "  history = model.fit(X_train, y_train, epochs =30, validation_data= (X_test, y_test),batch_size=64, verbose=1)  \n",
        "  y_pred = model.predict_classes(X_test)\n",
        "  mat = confusion_matrix(y_test, y_pred)\n",
        "  print(confusion_matrix(y_test, y_pred))\n",
        "  print(accuracy_score(y_test,y_pred))\n",
        "  print(classification_report(y_test,y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0001\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_7 (Conv2D)            (None, 200, 3, 64)        640       \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 200, 3, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 100, 1, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 6400)              0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 1024)              6554624   \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 6)                 6150      \n",
            "=================================================================\n",
            "Total params: 6,561,414\n",
            "Trainable params: 6,561,414\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 21879 samples, validate on 5470 samples\n",
            "Epoch 1/30\n",
            "21879/21879 [==============================] - 3s 158us/step - loss: 1.2947 - acc: 0.4525 - val_loss: 1.0169 - val_acc: 0.5967\n",
            "Epoch 2/30\n",
            "21879/21879 [==============================] - 3s 120us/step - loss: 0.9433 - acc: 0.5993 - val_loss: 0.9228 - val_acc: 0.6426\n",
            "Epoch 3/30\n",
            "21879/21879 [==============================] - 3s 121us/step - loss: 0.8639 - acc: 0.6333 - val_loss: 0.8702 - val_acc: 0.6592\n",
            "Epoch 4/30\n",
            "21879/21879 [==============================] - 3s 117us/step - loss: 0.8111 - acc: 0.6575 - val_loss: 0.8302 - val_acc: 0.6878\n",
            "Epoch 5/30\n",
            "21879/21879 [==============================] - 3s 116us/step - loss: 0.7646 - acc: 0.6840 - val_loss: 0.8060 - val_acc: 0.6952\n",
            "Epoch 6/30\n",
            "21879/21879 [==============================] - 3s 119us/step - loss: 0.7261 - acc: 0.7023 - val_loss: 0.7858 - val_acc: 0.6934\n",
            "Epoch 7/30\n",
            "21879/21879 [==============================] - 3s 114us/step - loss: 0.6936 - acc: 0.7148 - val_loss: 0.7714 - val_acc: 0.7013\n",
            "Epoch 8/30\n",
            "21879/21879 [==============================] - 3s 118us/step - loss: 0.6691 - acc: 0.7265 - val_loss: 0.7560 - val_acc: 0.7174\n",
            "Epoch 9/30\n",
            "21879/21879 [==============================] - 3s 117us/step - loss: 0.6410 - acc: 0.7384 - val_loss: 0.7360 - val_acc: 0.7144\n",
            "Epoch 10/30\n",
            "21879/21879 [==============================] - 3s 116us/step - loss: 0.6143 - acc: 0.7541 - val_loss: 0.7186 - val_acc: 0.7245\n",
            "Epoch 11/30\n",
            "21879/21879 [==============================] - 3s 116us/step - loss: 0.5930 - acc: 0.7618 - val_loss: 0.7159 - val_acc: 0.7260\n",
            "Epoch 12/30\n",
            "21879/21879 [==============================] - 3s 117us/step - loss: 0.5702 - acc: 0.7700 - val_loss: 0.7163 - val_acc: 0.7252\n",
            "Epoch 13/30\n",
            "21879/21879 [==============================] - 3s 116us/step - loss: 0.5544 - acc: 0.7785 - val_loss: 0.7054 - val_acc: 0.7377\n",
            "Epoch 14/30\n",
            "21879/21879 [==============================] - 3s 116us/step - loss: 0.5379 - acc: 0.7852 - val_loss: 0.7058 - val_acc: 0.7333\n",
            "Epoch 15/30\n",
            "21879/21879 [==============================] - 3s 115us/step - loss: 0.5220 - acc: 0.7934 - val_loss: 0.6902 - val_acc: 0.7430\n",
            "Epoch 16/30\n",
            "21879/21879 [==============================] - 3s 117us/step - loss: 0.4999 - acc: 0.8028 - val_loss: 0.6881 - val_acc: 0.7431\n",
            "Epoch 17/30\n",
            "21879/21879 [==============================] - 3s 116us/step - loss: 0.4924 - acc: 0.8051 - val_loss: 0.6842 - val_acc: 0.7530\n",
            "Epoch 18/30\n",
            "21879/21879 [==============================] - 3s 116us/step - loss: 0.4709 - acc: 0.8131 - val_loss: 0.6823 - val_acc: 0.7537\n",
            "Epoch 19/30\n",
            "21879/21879 [==============================] - 3s 117us/step - loss: 0.4569 - acc: 0.8174 - val_loss: 0.6824 - val_acc: 0.7536\n",
            "Epoch 20/30\n",
            "21879/21879 [==============================] - 2s 114us/step - loss: 0.4454 - acc: 0.8252 - val_loss: 0.6861 - val_acc: 0.7512\n",
            "Epoch 21/30\n",
            "21879/21879 [==============================] - 3s 114us/step - loss: 0.4347 - acc: 0.8275 - val_loss: 0.6829 - val_acc: 0.7550\n",
            "Epoch 22/30\n",
            "21879/21879 [==============================] - 3s 117us/step - loss: 0.4264 - acc: 0.8323 - val_loss: 0.6844 - val_acc: 0.7605\n",
            "Epoch 23/30\n",
            "21879/21879 [==============================] - 3s 117us/step - loss: 0.4096 - acc: 0.8399 - val_loss: 0.6843 - val_acc: 0.7618\n",
            "Epoch 24/30\n",
            "21879/21879 [==============================] - 3s 119us/step - loss: 0.4052 - acc: 0.8405 - val_loss: 0.6815 - val_acc: 0.7580\n",
            "Epoch 25/30\n",
            "21879/21879 [==============================] - 3s 114us/step - loss: 0.3903 - acc: 0.8441 - val_loss: 0.6818 - val_acc: 0.7600\n",
            "Epoch 26/30\n",
            "21879/21879 [==============================] - 3s 116us/step - loss: 0.3871 - acc: 0.8451 - val_loss: 0.6886 - val_acc: 0.7601\n",
            "Epoch 27/30\n",
            "21879/21879 [==============================] - 3s 115us/step - loss: 0.3696 - acc: 0.8544 - val_loss: 0.6816 - val_acc: 0.7684\n",
            "Epoch 28/30\n",
            "21879/21879 [==============================] - 3s 116us/step - loss: 0.3664 - acc: 0.8586 - val_loss: 0.6818 - val_acc: 0.7645\n",
            "Epoch 29/30\n",
            "21879/21879 [==============================] - 3s 116us/step - loss: 0.3530 - acc: 0.8618 - val_loss: 0.6916 - val_acc: 0.7658\n",
            "Epoch 30/30\n",
            "21879/21879 [==============================] - 3s 117us/step - loss: 0.3520 - acc: 0.8620 - val_loss: 0.6941 - val_acc: 0.7675\n",
            "[[988  13  22  10   7   5]\n",
            " [ 16 627   7   4 183   1]\n",
            " [ 32   1 653 136   1  34]\n",
            " [ 26   4 151 646   5  60]\n",
            " [ 32 251   8   9 552   9]\n",
            " [ 28  10  97 101   9 732]]\n",
            "0.7674588665447898\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.95      0.91      1045\n",
            "           1       0.69      0.75      0.72       838\n",
            "           2       0.70      0.76      0.73       857\n",
            "           3       0.71      0.72      0.72       892\n",
            "           4       0.73      0.64      0.68       861\n",
            "           5       0.87      0.75      0.81       977\n",
            "\n",
            "    accuracy                           0.77      5470\n",
            "   macro avg       0.76      0.76      0.76      5470\n",
            "weighted avg       0.77      0.77      0.77      5470\n",
            "\n",
            "0.001\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_8 (Conv2D)            (None, 200, 3, 64)        640       \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 200, 3, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 100, 1, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 6400)              0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 1024)              6554624   \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 6)                 6150      \n",
            "=================================================================\n",
            "Total params: 6,561,414\n",
            "Trainable params: 6,561,414\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 21879 samples, validate on 5470 samples\n",
            "Epoch 1/30\n",
            "21879/21879 [==============================] - 3s 158us/step - loss: 1.0830 - acc: 0.5336 - val_loss: 0.8859 - val_acc: 0.6196\n",
            "Epoch 2/30\n",
            "21879/21879 [==============================] - 3s 117us/step - loss: 0.8462 - acc: 0.6255 - val_loss: 0.8197 - val_acc: 0.6605\n",
            "Epoch 3/30\n",
            "21879/21879 [==============================] - 3s 118us/step - loss: 0.7708 - acc: 0.6651 - val_loss: 0.7590 - val_acc: 0.6914\n",
            "Epoch 4/30\n",
            "21879/21879 [==============================] - 3s 119us/step - loss: 0.7093 - acc: 0.6962 - val_loss: 0.7379 - val_acc: 0.6973\n",
            "Epoch 5/30\n",
            "21879/21879 [==============================] - 3s 119us/step - loss: 0.6633 - acc: 0.7166 - val_loss: 0.7164 - val_acc: 0.7225\n",
            "Epoch 6/30\n",
            "21879/21879 [==============================] - 3s 121us/step - loss: 0.6178 - acc: 0.7413 - val_loss: 0.6871 - val_acc: 0.7238\n",
            "Epoch 7/30\n",
            "21879/21879 [==============================] - 3s 119us/step - loss: 0.5830 - acc: 0.7602 - val_loss: 0.6723 - val_acc: 0.7369\n",
            "Epoch 8/30\n",
            "21879/21879 [==============================] - 3s 119us/step - loss: 0.5424 - acc: 0.7748 - val_loss: 0.6606 - val_acc: 0.7466\n",
            "Epoch 9/30\n",
            "21879/21879 [==============================] - 3s 119us/step - loss: 0.5172 - acc: 0.7915 - val_loss: 0.6668 - val_acc: 0.7375\n",
            "Epoch 10/30\n",
            "21879/21879 [==============================] - 3s 120us/step - loss: 0.4901 - acc: 0.7994 - val_loss: 0.6565 - val_acc: 0.7525\n",
            "Epoch 11/30\n",
            "21879/21879 [==============================] - 3s 119us/step - loss: 0.4681 - acc: 0.8108 - val_loss: 0.6419 - val_acc: 0.7561\n",
            "Epoch 12/30\n",
            "21879/21879 [==============================] - 3s 120us/step - loss: 0.4465 - acc: 0.8181 - val_loss: 0.6556 - val_acc: 0.7550\n",
            "Epoch 13/30\n",
            "21879/21879 [==============================] - 3s 120us/step - loss: 0.4328 - acc: 0.8265 - val_loss: 0.6556 - val_acc: 0.7523\n",
            "Epoch 14/30\n",
            "21879/21879 [==============================] - 3s 118us/step - loss: 0.4156 - acc: 0.8323 - val_loss: 0.6596 - val_acc: 0.7548\n",
            "Epoch 15/30\n",
            "21879/21879 [==============================] - 3s 118us/step - loss: 0.3967 - acc: 0.8419 - val_loss: 0.6584 - val_acc: 0.7512\n",
            "Epoch 16/30\n",
            "21879/21879 [==============================] - 3s 120us/step - loss: 0.3831 - acc: 0.8443 - val_loss: 0.6640 - val_acc: 0.7530\n",
            "Epoch 17/30\n",
            "21879/21879 [==============================] - 3s 123us/step - loss: 0.3705 - acc: 0.8500 - val_loss: 0.6743 - val_acc: 0.7605\n",
            "Epoch 18/30\n",
            "21879/21879 [==============================] - 3s 123us/step - loss: 0.3583 - acc: 0.8580 - val_loss: 0.6946 - val_acc: 0.7552\n",
            "Epoch 19/30\n",
            "21879/21879 [==============================] - 3s 125us/step - loss: 0.3498 - acc: 0.8607 - val_loss: 0.6744 - val_acc: 0.7669\n",
            "Epoch 20/30\n",
            "21879/21879 [==============================] - 3s 124us/step - loss: 0.3336 - acc: 0.8680 - val_loss: 0.7054 - val_acc: 0.7620\n",
            "Epoch 21/30\n",
            "21879/21879 [==============================] - 3s 119us/step - loss: 0.3334 - acc: 0.8677 - val_loss: 0.6918 - val_acc: 0.7693\n",
            "Epoch 22/30\n",
            "21879/21879 [==============================] - 3s 118us/step - loss: 0.3239 - acc: 0.8714 - val_loss: 0.7226 - val_acc: 0.7603\n",
            "Epoch 23/30\n",
            "21879/21879 [==============================] - 3s 118us/step - loss: 0.3210 - acc: 0.8756 - val_loss: 0.7085 - val_acc: 0.7601\n",
            "Epoch 24/30\n",
            "21879/21879 [==============================] - 3s 117us/step - loss: 0.3107 - acc: 0.8763 - val_loss: 0.7344 - val_acc: 0.7684\n",
            "Epoch 25/30\n",
            "21879/21879 [==============================] - 3s 118us/step - loss: 0.3000 - acc: 0.8813 - val_loss: 0.7304 - val_acc: 0.7653\n",
            "Epoch 26/30\n",
            "21879/21879 [==============================] - 3s 119us/step - loss: 0.2921 - acc: 0.8823 - val_loss: 0.7466 - val_acc: 0.7607\n",
            "Epoch 27/30\n",
            "21879/21879 [==============================] - 3s 119us/step - loss: 0.2912 - acc: 0.8851 - val_loss: 0.7299 - val_acc: 0.7600\n",
            "Epoch 28/30\n",
            "21879/21879 [==============================] - 3s 118us/step - loss: 0.2764 - acc: 0.8890 - val_loss: 0.7458 - val_acc: 0.7755\n",
            "Epoch 29/30\n",
            "21879/21879 [==============================] - 3s 120us/step - loss: 0.2871 - acc: 0.8878 - val_loss: 0.7794 - val_acc: 0.7709\n",
            "Epoch 30/30\n",
            "21879/21879 [==============================] - 3s 120us/step - loss: 0.2866 - acc: 0.8876 - val_loss: 0.7712 - val_acc: 0.7715\n",
            "[[984  16  17   8  11   9]\n",
            " [ 13 692   4   2 124   3]\n",
            " [ 17   3 603 144  10  80]\n",
            " [ 17   5 138 635   9  88]\n",
            " [ 19 289   5   7 528  13]\n",
            " [ 21  15  77  73  13 778]]\n",
            "0.7714808043875686\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.94      0.93      1045\n",
            "           1       0.68      0.83      0.74       838\n",
            "           2       0.71      0.70      0.71       857\n",
            "           3       0.73      0.71      0.72       892\n",
            "           4       0.76      0.61      0.68       861\n",
            "           5       0.80      0.80      0.80       977\n",
            "\n",
            "    accuracy                           0.77      5470\n",
            "   macro avg       0.77      0.77      0.76      5470\n",
            "weighted avg       0.77      0.77      0.77      5470\n",
            "\n",
            "0.01\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_9 (Conv2D)            (None, 200, 3, 64)        640       \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 200, 3, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 100, 1, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 6400)              0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 1024)              6554624   \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 6)                 6150      \n",
            "=================================================================\n",
            "Total params: 6,561,414\n",
            "Trainable params: 6,561,414\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 21879 samples, validate on 5470 samples\n",
            "Epoch 1/30\n",
            "21879/21879 [==============================] - 4s 171us/step - loss: 1.3054 - acc: 0.5077 - val_loss: 0.9540 - val_acc: 0.5631\n",
            "Epoch 2/30\n",
            "21879/21879 [==============================] - 3s 119us/step - loss: 1.0336 - acc: 0.5464 - val_loss: 1.0053 - val_acc: 0.5706\n",
            "Epoch 3/30\n",
            "21879/21879 [==============================] - 3s 120us/step - loss: 1.0261 - acc: 0.5593 - val_loss: 0.8903 - val_acc: 0.5918\n",
            "Epoch 4/30\n",
            "21879/21879 [==============================] - 3s 118us/step - loss: 1.0001 - acc: 0.5634 - val_loss: 0.8832 - val_acc: 0.6234\n",
            "Epoch 5/30\n",
            "21879/21879 [==============================] - 3s 121us/step - loss: 0.9809 - acc: 0.5695 - val_loss: 0.8880 - val_acc: 0.6037\n",
            "Epoch 6/30\n",
            "21879/21879 [==============================] - 3s 118us/step - loss: 0.9605 - acc: 0.5748 - val_loss: 0.9616 - val_acc: 0.5830\n",
            "Epoch 7/30\n",
            "21879/21879 [==============================] - 3s 121us/step - loss: 0.9447 - acc: 0.5849 - val_loss: 0.9807 - val_acc: 0.5545\n",
            "Epoch 8/30\n",
            "21879/21879 [==============================] - 3s 121us/step - loss: 0.9279 - acc: 0.5953 - val_loss: 0.8626 - val_acc: 0.6282\n",
            "Epoch 9/30\n",
            "21879/21879 [==============================] - 3s 120us/step - loss: 0.9396 - acc: 0.5899 - val_loss: 0.8962 - val_acc: 0.6236\n",
            "Epoch 10/30\n",
            "21879/21879 [==============================] - 3s 123us/step - loss: 0.9241 - acc: 0.5917 - val_loss: 0.8846 - val_acc: 0.6117\n",
            "Epoch 11/30\n",
            "21879/21879 [==============================] - 3s 123us/step - loss: 0.9194 - acc: 0.5965 - val_loss: 0.8580 - val_acc: 0.6263\n",
            "Epoch 12/30\n",
            "21879/21879 [==============================] - 3s 121us/step - loss: 0.8829 - acc: 0.6058 - val_loss: 0.9237 - val_acc: 0.5976\n",
            "Epoch 13/30\n",
            "21879/21879 [==============================] - 3s 121us/step - loss: 0.9033 - acc: 0.5986 - val_loss: 0.8978 - val_acc: 0.6214\n",
            "Epoch 14/30\n",
            "21879/21879 [==============================] - 3s 123us/step - loss: 0.8960 - acc: 0.6059 - val_loss: 0.8693 - val_acc: 0.6221\n",
            "Epoch 15/30\n",
            "21879/21879 [==============================] - 3s 125us/step - loss: 0.8885 - acc: 0.6082 - val_loss: 0.8635 - val_acc: 0.6305\n",
            "Epoch 16/30\n",
            "21879/21879 [==============================] - 3s 118us/step - loss: 0.8784 - acc: 0.6101 - val_loss: 0.8481 - val_acc: 0.6459\n",
            "Epoch 17/30\n",
            "21879/21879 [==============================] - 3s 117us/step - loss: 0.8467 - acc: 0.6259 - val_loss: 0.8833 - val_acc: 0.6053\n",
            "Epoch 18/30\n",
            "21879/21879 [==============================] - 3s 119us/step - loss: 0.8698 - acc: 0.6137 - val_loss: 0.8451 - val_acc: 0.6353\n",
            "Epoch 19/30\n",
            "21879/21879 [==============================] - 3s 117us/step - loss: 0.8763 - acc: 0.6112 - val_loss: 0.9137 - val_acc: 0.6243\n",
            "Epoch 20/30\n",
            "21879/21879 [==============================] - 3s 119us/step - loss: 0.8584 - acc: 0.6201 - val_loss: 0.8813 - val_acc: 0.6185\n",
            "Epoch 21/30\n",
            "21879/21879 [==============================] - 3s 121us/step - loss: 0.8687 - acc: 0.6189 - val_loss: 0.8390 - val_acc: 0.6603\n",
            "Epoch 22/30\n",
            "21879/21879 [==============================] - 3s 119us/step - loss: 0.8458 - acc: 0.6240 - val_loss: 0.8458 - val_acc: 0.6530\n",
            "Epoch 23/30\n",
            "21879/21879 [==============================] - 3s 118us/step - loss: 0.8627 - acc: 0.6163 - val_loss: 0.8924 - val_acc: 0.6340\n",
            "Epoch 24/30\n",
            "21879/21879 [==============================] - 3s 118us/step - loss: 0.8575 - acc: 0.6190 - val_loss: 0.8595 - val_acc: 0.6417\n",
            "Epoch 25/30\n",
            "21879/21879 [==============================] - 3s 120us/step - loss: 0.8559 - acc: 0.6205 - val_loss: 0.8775 - val_acc: 0.6360\n",
            "Epoch 26/30\n",
            "21879/21879 [==============================] - 3s 119us/step - loss: 0.8205 - acc: 0.6372 - val_loss: 0.8613 - val_acc: 0.6545\n",
            "Epoch 27/30\n",
            "21879/21879 [==============================] - 3s 120us/step - loss: 0.8317 - acc: 0.6328 - val_loss: 0.8719 - val_acc: 0.6437\n",
            "Epoch 28/30\n",
            "21879/21879 [==============================] - 3s 119us/step - loss: 0.8578 - acc: 0.6284 - val_loss: 0.9274 - val_acc: 0.6139\n",
            "Epoch 29/30\n",
            "21879/21879 [==============================] - 3s 118us/step - loss: 0.8261 - acc: 0.6286 - val_loss: 0.8699 - val_acc: 0.6437\n",
            "Epoch 30/30\n",
            "21879/21879 [==============================] - 3s 120us/step - loss: 0.8158 - acc: 0.6374 - val_loss: 0.8779 - val_acc: 0.6320\n",
            "[[944   4  64  10  13  10]\n",
            " [ 15 414  13   8 377  11]\n",
            " [ 11   0 758  52   4  32]\n",
            " [ 15   3 588 242   6  38]\n",
            " [ 15 195  33  13 586  19]\n",
            " [  7   6 364  77  10 513]]\n",
            "0.6319926873857404\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.90      0.92      1045\n",
            "           1       0.67      0.49      0.57       838\n",
            "           2       0.42      0.88      0.57       857\n",
            "           3       0.60      0.27      0.37       892\n",
            "           4       0.59      0.68      0.63       861\n",
            "           5       0.82      0.53      0.64       977\n",
            "\n",
            "    accuracy                           0.63      5470\n",
            "   macro avg       0.67      0.63      0.62      5470\n",
            "weighted avg       0.68      0.63      0.63      5470\n",
            "\n",
            "0.1\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_10 (Conv2D)           (None, 200, 3, 64)        640       \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 200, 3, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 100, 1, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 6400)              0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 1024)              6554624   \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 6)                 6150      \n",
            "=================================================================\n",
            "Total params: 6,561,414\n",
            "Trainable params: 6,561,414\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 21879 samples, validate on 5470 samples\n",
            "Epoch 1/30\n",
            "21879/21879 [==============================] - 4s 176us/step - loss: 12.9923 - acc: 0.1919 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 2/30\n",
            "21879/21879 [==============================] - 3s 119us/step - loss: 13.0380 - acc: 0.1911 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 3/30\n",
            "21879/21879 [==============================] - 3s 118us/step - loss: 13.0380 - acc: 0.1911 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 4/30\n",
            "21879/21879 [==============================] - 3s 119us/step - loss: 13.0380 - acc: 0.1911 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 5/30\n",
            "21879/21879 [==============================] - 3s 119us/step - loss: 13.0380 - acc: 0.1911 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 6/30\n",
            "21879/21879 [==============================] - 3s 119us/step - loss: 13.0380 - acc: 0.1911 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 7/30\n",
            "21879/21879 [==============================] - 3s 117us/step - loss: 13.0380 - acc: 0.1911 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 8/30\n",
            "21879/21879 [==============================] - 3s 115us/step - loss: 13.0380 - acc: 0.1911 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 9/30\n",
            "21879/21879 [==============================] - 3s 115us/step - loss: 13.0380 - acc: 0.1911 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 10/30\n",
            "21879/21879 [==============================] - 3s 115us/step - loss: 13.0380 - acc: 0.1911 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 11/30\n",
            "21879/21879 [==============================] - 3s 114us/step - loss: 13.0380 - acc: 0.1911 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 12/30\n",
            "21879/21879 [==============================] - 2s 113us/step - loss: 13.0380 - acc: 0.1911 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 13/30\n",
            "21879/21879 [==============================] - 3s 115us/step - loss: 13.0380 - acc: 0.1911 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 14/30\n",
            "21879/21879 [==============================] - 3s 115us/step - loss: 13.0380 - acc: 0.1911 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 15/30\n",
            "21879/21879 [==============================] - 2s 114us/step - loss: 13.0380 - acc: 0.1911 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 16/30\n",
            "21879/21879 [==============================] - 2s 114us/step - loss: 13.0380 - acc: 0.1911 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 17/30\n",
            "21879/21879 [==============================] - 3s 115us/step - loss: 13.0380 - acc: 0.1911 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 18/30\n",
            "21879/21879 [==============================] - 3s 115us/step - loss: 13.0380 - acc: 0.1911 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 19/30\n",
            "21879/21879 [==============================] - 2s 114us/step - loss: 13.0380 - acc: 0.1911 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 20/30\n",
            "21879/21879 [==============================] - 3s 115us/step - loss: 13.0380 - acc: 0.1911 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 21/30\n",
            "21879/21879 [==============================] - 3s 117us/step - loss: 13.0380 - acc: 0.1911 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 22/30\n",
            "21879/21879 [==============================] - 3s 121us/step - loss: 13.0380 - acc: 0.1911 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 23/30\n",
            "21879/21879 [==============================] - 3s 122us/step - loss: 13.0380 - acc: 0.1911 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 24/30\n",
            "21879/21879 [==============================] - 3s 121us/step - loss: 13.0380 - acc: 0.1911 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 25/30\n",
            "21879/21879 [==============================] - 3s 120us/step - loss: 13.0380 - acc: 0.1911 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 26/30\n",
            "21879/21879 [==============================] - 3s 119us/step - loss: 13.0380 - acc: 0.1911 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 27/30\n",
            "21879/21879 [==============================] - 3s 121us/step - loss: 13.0380 - acc: 0.1911 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 28/30\n",
            "21879/21879 [==============================] - 3s 124us/step - loss: 13.0380 - acc: 0.1911 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 29/30\n",
            "21879/21879 [==============================] - 3s 120us/step - loss: 13.0380 - acc: 0.1911 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 30/30\n",
            "21879/21879 [==============================] - 3s 119us/step - loss: 13.0380 - acc: 0.1911 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "[[1045    0    0    0    0    0]\n",
            " [ 838    0    0    0    0    0]\n",
            " [ 857    0    0    0    0    0]\n",
            " [ 892    0    0    0    0    0]\n",
            " [ 861    0    0    0    0    0]\n",
            " [ 977    0    0    0    0    0]]\n",
            "0.1910420475319927\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.19      1.00      0.32      1045\n",
            "           1       0.00      0.00      0.00       838\n",
            "           2       0.00      0.00      0.00       857\n",
            "           3       0.00      0.00      0.00       892\n",
            "           4       0.00      0.00      0.00       861\n",
            "           5       0.00      0.00      0.00       977\n",
            "\n",
            "    accuracy                           0.19      5470\n",
            "   macro avg       0.03      0.17      0.05      5470\n",
            "weighted avg       0.04      0.19      0.06      5470\n",
            "\n",
            "1.0\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_11 (Conv2D)           (None, 200, 3, 64)        640       \n",
            "_________________________________________________________________\n",
            "dropout_21 (Dropout)         (None, 200, 3, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 100, 1, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_11 (Flatten)         (None, 6400)              0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 1024)              6554624   \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 6)                 6150      \n",
            "=================================================================\n",
            "Total params: 6,561,414\n",
            "Trainable params: 6,561,414\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 21879 samples, validate on 5470 samples\n",
            "Epoch 1/30\n",
            "21879/21879 [==============================] - 4s 184us/step - loss: 13.0045 - acc: 0.1911 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 2/30\n",
            "21879/21879 [==============================] - 3s 120us/step - loss: 13.0439 - acc: 0.1907 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 3/30\n",
            "21879/21879 [==============================] - 3s 119us/step - loss: 13.0358 - acc: 0.1912 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 4/30\n",
            "21879/21879 [==============================] - 3s 120us/step - loss: 13.0358 - acc: 0.1912 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 5/30\n",
            "21879/21879 [==============================] - 3s 120us/step - loss: 13.0299 - acc: 0.1916 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 6/30\n",
            "21879/21879 [==============================] - 3s 119us/step - loss: 13.0255 - acc: 0.1919 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 7/30\n",
            "21879/21879 [==============================] - 3s 120us/step - loss: 13.0372 - acc: 0.1911 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 8/30\n",
            "21879/21879 [==============================] - 3s 120us/step - loss: 13.0255 - acc: 0.1919 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 9/30\n",
            "21879/21879 [==============================] - 3s 116us/step - loss: 13.0358 - acc: 0.1912 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 10/30\n",
            "21879/21879 [==============================] - 3s 120us/step - loss: 13.0291 - acc: 0.1916 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 11/30\n",
            "21879/21879 [==============================] - 3s 117us/step - loss: 13.0225 - acc: 0.1921 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 12/30\n",
            "21879/21879 [==============================] - 3s 116us/step - loss: 13.0159 - acc: 0.1925 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 13/30\n",
            "21879/21879 [==============================] - 3s 116us/step - loss: 13.0365 - acc: 0.1912 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 14/30\n",
            "21879/21879 [==============================] - 3s 119us/step - loss: 13.0350 - acc: 0.1913 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 15/30\n",
            "21879/21879 [==============================] - 3s 120us/step - loss: 13.0387 - acc: 0.1911 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 16/30\n",
            "21879/21879 [==============================] - 3s 120us/step - loss: 13.0372 - acc: 0.1911 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 17/30\n",
            "21879/21879 [==============================] - 3s 130us/step - loss: 13.0262 - acc: 0.1918 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 18/30\n",
            "21879/21879 [==============================] - 3s 118us/step - loss: 13.0424 - acc: 0.1908 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 19/30\n",
            "21879/21879 [==============================] - 3s 119us/step - loss: 13.0365 - acc: 0.1912 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 20/30\n",
            "21879/21879 [==============================] - 3s 117us/step - loss: 13.0372 - acc: 0.1911 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 21/30\n",
            "21879/21879 [==============================] - 3s 121us/step - loss: 13.0358 - acc: 0.1912 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 22/30\n",
            "21879/21879 [==============================] - 3s 121us/step - loss: 13.0380 - acc: 0.1911 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 23/30\n",
            "21879/21879 [==============================] - 3s 118us/step - loss: 13.0321 - acc: 0.1915 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 24/30\n",
            "21879/21879 [==============================] - 3s 119us/step - loss: 13.0328 - acc: 0.1914 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 25/30\n",
            "21879/21879 [==============================] - 3s 118us/step - loss: 13.0350 - acc: 0.1913 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 26/30\n",
            "21879/21879 [==============================] - 3s 119us/step - loss: 13.0372 - acc: 0.1911 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 27/30\n",
            "21879/21879 [==============================] - 3s 118us/step - loss: 13.0328 - acc: 0.1914 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 28/30\n",
            "21879/21879 [==============================] - 3s 119us/step - loss: 13.0350 - acc: 0.1913 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 29/30\n",
            "21879/21879 [==============================] - 3s 118us/step - loss: 13.0343 - acc: 0.1913 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "Epoch 30/30\n",
            "21879/21879 [==============================] - 3s 119us/step - loss: 13.0306 - acc: 0.1916 - val_loss: 13.0389 - val_acc: 0.1910\n",
            "[[1045    0    0    0    0    0]\n",
            " [ 838    0    0    0    0    0]\n",
            " [ 857    0    0    0    0    0]\n",
            " [ 892    0    0    0    0    0]\n",
            " [ 861    0    0    0    0    0]\n",
            " [ 977    0    0    0    0    0]]\n",
            "0.1910420475319927\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.19      1.00      0.32      1045\n",
            "           1       0.00      0.00      0.00       838\n",
            "           2       0.00      0.00      0.00       857\n",
            "           3       0.00      0.00      0.00       892\n",
            "           4       0.00      0.00      0.00       861\n",
            "           5       0.00      0.00      0.00       977\n",
            "\n",
            "    accuracy                           0.19      5470\n",
            "   macro avg       0.03      0.17      0.05      5470\n",
            "weighted avg       0.04      0.19      0.06      5470\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}